{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instalación de librerías externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.15.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.0->transformers)\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/9.1 MB 6.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/9.1 MB 11.4 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/9.1 MB 17.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.7/9.1 MB 21.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.8/9.1 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.1 MB 29.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.1 MB 29.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.1 MB 20.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 23.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 401.7/401.7 kB 12.2 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl (268 kB)\n",
      "Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 287.4/287.4 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.1/2.2 MB 34.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 28.3 MB/s eta 0:00:00\n",
      "Downloading filelock-3.15.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "   ---------------------------------------- 0.0/176.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 176.9/176.9 kB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, pyyaml, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.15.1 fsspec-2024.6.0 huggingface-hub-0.23.3 pyyaml-6.0.1 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 122.9/991.5 kB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 604.2/991.5 kB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 991.5/991.5 kB 10.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Downloading torch-2.3.1-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from accelerate) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.0)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.6.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch>=1.10.0->accelerate)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate)\n",
      "  Downloading tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch>=1.10.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "   ---------------------------------------- 0.0/309.4 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 194.6/309.4 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 309.4/309.4 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading torch-2.3.1-cp310-cp310-win_amd64.whl (159.8 MB)\n",
      "   ---------------------------------------- 0.0/159.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.7/159.8 MB 23.8 MB/s eta 0:00:07\n",
      "    --------------------------------------- 2.3/159.8 MB 28.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 4.4/159.8 MB 35.1 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 6.4/159.8 MB 37.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 7.9/159.8 MB 36.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 10.5/159.8 MB 38.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 12.1/159.8 MB 40.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 14.3/159.8 MB 40.9 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 16.4/159.8 MB 43.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 18.8/159.8 MB 46.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 21.0/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 23.3/159.8 MB 50.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 25.6/159.8 MB 50.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 27.6/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 29.8/159.8 MB 50.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 32.1/159.8 MB 46.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 34.5/159.8 MB 46.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 36.9/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 39.1/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 40.5/159.8 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 42.5/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 44.8/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 46.6/159.8 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 48.9/159.8 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 51.2/159.8 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 53.7/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 54.8/159.8 MB 40.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 57.3/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 59.6/159.8 MB 43.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 61.6/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 64.0/159.8 MB 46.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 66.3/159.8 MB 46.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 68.5/159.8 MB 46.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 70.7/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 73.1/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 75.2/159.8 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 77.6/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 79.5/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 82.1/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 84.1/159.8 MB 50.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 86.7/159.8 MB 46.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 88.9/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 90.7/159.8 MB 50.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 93.1/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 95.0/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 97.6/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 99.7/159.8 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 101.6/159.8 MB 46.9 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 103.8/159.8 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 105.3/159.8 MB 43.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 107.3/159.8 MB 40.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 109.8/159.8 MB 40.9 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 111.0/159.8 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 113.3/159.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 114.7/159.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 116.7/159.8 MB 43.5 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 119.3/159.8 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 121.4/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 123.7/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 125.7/159.8 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 128.2/159.8 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 129.6/159.8 MB 43.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 132.1/159.8 MB 43.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 134.0/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 136.2/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 138.7/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 141.2/159.8 MB 46.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 143.5/159.8 MB 50.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.5/159.8 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 147.7/159.8 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 149.4/159.8 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 151.3/159.8 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 154.0/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  156.3/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  158.4/159.8 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 46.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 46.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 46.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 46.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 46.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.8/159.8 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/228.5 MB 53.8 MB/s eta 0:00:05\n",
      "    --------------------------------------- 4.1/228.5 MB 52.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 6.3/228.5 MB 50.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 8.6/228.5 MB 50.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 10.9/228.5 MB 50.4 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 12.3/228.5 MB 46.7 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 14.4/228.5 MB 43.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 15.4/228.5 MB 43.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 17.5/228.5 MB 40.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 18.0/228.5 MB 36.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 18.0/228.5 MB 36.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 19.8/228.5 MB 28.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 23.1/228.5 MB 14.6 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 24.0/228.5 MB 14.2 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 26.7/228.5 MB 14.9 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 29.0/228.5 MB 17.7 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 31.3/228.5 MB 16.8 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 32.9/228.5 MB 40.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 35.1/228.5 MB 46.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 37.7/228.5 MB 46.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 40.1/228.5 MB 50.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 42.4/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 44.5/228.5 MB 50.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 47.0/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 49.3/228.5 MB 54.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 51.5/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 53.6/228.5 MB 54.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 55.8/228.5 MB 46.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 58.0/228.5 MB 46.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 58.0/228.5 MB 46.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 58.0/228.5 MB 46.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 62.6/228.5 MB 40.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 64.9/228.5 MB 40.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 65.6/228.5 MB 38.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 66.9/228.5 MB 36.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 69.5/228.5 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 71.9/228.5 MB 43.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 74.2/228.5 MB 38.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 75.8/228.5 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 78.1/228.5 MB 46.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 80.0/228.5 MB 43.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 82.6/228.5 MB 46.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 84.7/228.5 MB 46.9 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 87.0/228.5 MB 46.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 88.6/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 91.0/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 93.7/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 96.1/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 96.9/228.5 MB 43.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 98.9/228.5 MB 43.7 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 101.6/228.5 MB 43.7 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 103.9/228.5 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 106.4/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 108.3/228.5 MB 50.4 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 110.5/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 113.4/228.5 MB 50.4 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 115.1/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 117.7/228.5 MB 50.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 120.0/228.5 MB 50.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 122.3/228.5 MB 50.4 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 124.4/228.5 MB 46.7 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 126.6/228.5 MB 50.4 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 128.3/228.5 MB 50.4 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 130.1/228.5 MB 43.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 131.9/228.5 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 134.3/228.5 MB 40.9 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 135.2/228.5 MB 40.9 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 137.1/228.5 MB 38.5 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 139.5/228.5 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 142.2/228.5 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 143.4/228.5 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 145.2/228.5 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 146.8/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 148.9/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 150.9/228.5 MB 38.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 153.0/228.5 MB 38.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 155.1/228.5 MB 43.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 157.6/228.5 MB 46.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 159.8/228.5 MB 46.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 162.0/228.5 MB 46.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 163.1/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 165.2/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 167.3/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 169.4/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 171.2/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 173.6/228.5 MB 46.9 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 174.8/228.5 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 177.1/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 178.6/228.5 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 180.0/228.5 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 182.8/228.5 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 184.0/228.5 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 185.6/228.5 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 187.4/228.5 MB 38.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 190.0/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 192.0/228.5 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 194.0/228.5 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 195.2/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 197.1/228.5 MB 43.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 199.4/228.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 201.1/228.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 203.3/228.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 205.6/228.5 MB 46.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 206.8/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 208.9/228.5 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 211.3/228.5 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 213.5/228.5 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 215.6/228.5 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 217.6/228.5 MB 46.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 219.9/228.5 MB 46.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 221.8/228.5 MB 46.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  223.9/228.5 MB 46.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  226.1/228.5 MB 46.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  227.6/228.5 MB 43.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 2.1/3.5 MB 44.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 MB 44.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 MB 44.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 22.5 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.4/286.4 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.9/5.7 MB 40.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.3/5.7 MB 45.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 40.7 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 16.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, mkl, torch, accelerate\n",
      "Successfully installed accelerate-0.31.0 intel-openmp-2021.4.0 mkl-2021.4.0 mpmath-1.3.0 sympy-1.12.1 tbb-2021.12.0 torch-2.3.1\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (0.23.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "   ---------------------------------------- 0.0/547.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 225.3/547.8 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  542.7/547.8 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 547.8/547.8 kB 5.7 MB/s eta 0:00:00\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-win_amd64.whl (25.9 MB)\n",
      "   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.9 MB 16.8 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.1/25.9 MB 22.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 3.9/25.9 MB 27.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 6.1/25.9 MB 32.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.2/25.9 MB 34.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.2/25.9 MB 36.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.7/25.9 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 12.8/25.9 MB 38.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.1/25.9 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.3/25.9 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.5/25.9 MB 46.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.7/25.9 MB 46.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.8/25.9 MB 46.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.8/25.9 MB 46.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.7/25.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.6/25.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.9/25.9 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (0.23.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.1/84.1 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from nltk->rouge_score) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\santinr\\mplace_meli\\venv_meli\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 112.6/133.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.7/133.7 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 16.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=7943a1a9f57237218e8c149ce484f9d89ad5919a5dd6b75144432d4a300cb4e4\n",
      "  Stored in directory: c:\\users\\santinr\\appdata\\local\\pip\\cache\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, nltk, rouge_score\n",
      "Successfully installed absl-py-2.1.0 nltk-3.8.1 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comportamiento de [Flan-T5-small](https://huggingface.co/google/flan-t5-small) sin Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del modelo y tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Verificar si MPS (Metal Performance Shaders) está disponible y usarlo si es posible\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Importar el tokenizador\n",
    "tokenizer_FT5 = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Importar el modelo pre-entrenado\n",
    "model_FT5 = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selección y preparación del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Carga de los Conjuntos de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    marketplace: Dataset({\n",
      "        features: ['title', 'price', 'original_price', 'CATEGORIA_CODE'],\n",
      "        num_rows: 912102\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Cargar los datos\n",
    "good_data = load_dataset('csv', data_files='../data/dataset_llm.csv')\n",
    "\n",
    "good_df = good_df[['title', 'price', 'CATEGORIA_CODE']]\n",
    "good_df = good_df[good_df['CATEGORIA_CODE']=='MLA1000'].head(50)\n",
    "\n",
    "\n",
    "# # Cargar los datos malos\n",
    "# bad_data = load_dataset('csv', data_files='/Users/feliperangel/LLMs/mvp_0_Instruction_Fine_tuning_LLM_Base/data/ping_bad.csv')\n",
    "\n",
    "# Unir los conjuntos de datos en un DatasetDict\n",
    "datasets = DatasetDict({\n",
    "    'marketplace': good_data['train']\n",
    "})\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualización y Exploración de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title    price  original_price  \\\n",
      "0  Auriculares Bluetooth In-ear Gamer F9-5 Negro ...   7599.0          9999.0   \n",
      "1      Auriculares Inalámbricos Jbl Tune 520bt Negro  68999.0             NaN   \n",
      "2  Auriculares In-ear Inalámbricos Xiaomi Redmi B...  33894.0             NaN   \n",
      "3  Parlante Philco Djp11p Portátil Con Bluetooth ...  59999.0         78999.0   \n",
      "4  Pila Aa Energizer Max E91 Cilíndrica - Pack De...   4200.0             NaN   \n",
      "\n",
      "  CATEGORIA_CODE  \n",
      "0        MLA1000  \n",
      "1        MLA1000  \n",
      "2        MLA1000  \n",
      "3        MLA1000  \n",
      "4        MLA1000  \n"
     ]
    }
   ],
   "source": [
    "# Mostrar una muestra de los datos buenos\n",
    "print(datasets['marketplace'].to_pandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Preprocesamiento y Manipulación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 6242.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'price', 'CATEGORIA_CODE', '__index_level_0__', 'label']\n",
      "Dataset({\n",
      "    features: ['title', 'price', 'CATEGORIA_CODE', '__index_level_0__', 'label'],\n",
      "    num_rows: 50\n",
      "})\n",
      "                                               title    price CATEGORIA_CODE  \\\n",
      "0  Auriculares Bluetooth In-ear Gamer F9-5 Negro ...   7599.0        MLA1000   \n",
      "1      Auriculares Inalámbricos Jbl Tune 520bt Negro  68999.0        MLA1000   \n",
      "2  Auriculares In-ear Inalámbricos Xiaomi Redmi B...  33894.0        MLA1000   \n",
      "3  Parlante Philco Djp11p Portátil Con Bluetooth ...  59999.0        MLA1000   \n",
      "4  Pila Aa Energizer Max E91 Cilíndrica - Pack De...   4200.0        MLA1000   \n",
      "\n",
      "   __index_level_0__  label  \n",
      "0                  0      1  \n",
      "1                  1      1  \n",
      "2                  2      1  \n",
      "3                  3      1  \n",
      "4                  4      1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "# Cargar los datos buenos\n",
    "good_df = pd.read_csv('../data/dataset_llm.csv', usecols=['title', 'price', 'CATEGORIA_CODE'])\n",
    "\n",
    "good_df = good_df[good_df['CATEGORIA_CODE']=='MLA1000'].head(50)\n",
    "\n",
    "# Asegurarse de que fltvalue sea de tipo float64\n",
    "good_df['price'] = good_df['price'].astype('float64')\n",
    "\n",
    "# Convertir el DataFrame a Dataset\n",
    "good_data = Dataset.from_pandas(good_df)\n",
    "\n",
    "# # Cargar los datos malos\n",
    "# bad_df = pd.read_csv('../data/dataset_llm.csv')\n",
    "# # Asegurarse de que fltvalue sea de tipo float64\n",
    "# bad_df['fltvalue'] = bad_df['fltvalue'].astype('float64')\n",
    "\n",
    "# # Convertir el DataFrame a Dataset\n",
    "# bad_data = Dataset.from_pandas(bad_df)\n",
    "\n",
    "# Crear un DatasetDict\n",
    "datasets = DatasetDict({\n",
    "    'marketplace': good_data,\n",
    "})\n",
    "\n",
    "# Función para etiquetar los datos\n",
    "def label_data(example, label):\n",
    "    example['label'] = label\n",
    "    return example\n",
    "\n",
    "# Etiquetar los datos buenos con '1' y los malos con '0'\n",
    "datasets['marketplace'] = datasets['marketplace'].map(lambda x: label_data(x, 1), batched=False)\n",
    "#datasets['bad'] = datasets['bad'].map(lambda x: label_data(x, 0), batched=False)\n",
    "\n",
    "# Unir los conjuntos etiquetados en un solo conjunto de datos\n",
    "all_data = concatenate_datasets([datasets['marketplace']\n",
    "                                 #, datasets['bad']\n",
    "                                 ])\n",
    "\n",
    "# Verificar la estructura y algunas muestras de los datos\n",
    "print(all_data.column_names)  # Debería incluir la nueva columna 'label'\n",
    "print(all_data)\n",
    "\n",
    "# Mostrar una muestra de los datos buenos y malos\n",
    "print(all_data.to_pandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 División en Entrenamiento y Prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'price', 'CATEGORIA_CODE', '__index_level_0__', 'label'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'price', 'CATEGORIA_CODE', '__index_level_0__', 'label'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos buenos y malos en entrenamiento y prueba\n",
    "train_test_split_good = datasets['marketplace'].train_test_split(test_size=0.2)\n",
    "# train_test_split_bad = datasets['bad'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Concatenar los conjuntos de entrenamiento y prueba\n",
    "# train_data = concatenate_datasets([train_test_split_good['train'], train_test_split_bad['train']])\n",
    "# test_data = concatenate_datasets([train_test_split_good['test'], train_test_split_bad['test']])\n",
    "train_data = train_test_split_good['train']\n",
    "test_data = train_test_split_good['test']\n",
    "\n",
    "# Crear un nuevo DatasetDict con las divisiones\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'test': test_data\n",
    "})\n",
    "\n",
    "print(split_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Formato del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "                                              prompt           completion\n",
      "0  Memorize the relationship between the 'title' ...  Electrónica y audio\n",
      "1  Memorize the relationship between the 'title' ...  Electrónica y audio\n",
      "2  Memorize the relationship between the 'title' ...  Electrónica y audio\n",
      "3  Memorize the relationship between the 'title' ...  Electrónica y audio\n",
      "4  Memorize the relationship between the 'title' ...  Electrónica y audio\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "# Cargar los datos buenos\n",
    "good_df = pd.read_csv('../data/dataset_llm.csv', usecols=['title', 'price', 'CATEGORIA_CODE'])\n",
    "good_df = good_df[good_df['CATEGORIA_CODE']=='MLA1000'].head(50)\n",
    "good_df['price'] = good_df['price'].astype('float64')\n",
    "good_df['label'] = 1\n",
    "\n",
    "# # Cargar los datos malos\n",
    "# bad_df = pd.read_csv('/Users/feliperangel/LLMs/mvp_0_Instruction_Fine_tuning_LLM_Base/data/ping_bad.csv')\n",
    "# bad_df['fltvalue'] = bad_df['fltvalue'].astype('float64')\n",
    "# bad_df['label'] = 0\n",
    "\n",
    "# Concatenar los datos\n",
    "df = good_df#pd.concat([good_df, bad_df])\n",
    "\n",
    "# Crear el prompt y el completion\n",
    "def create_prompt_completion(row):\n",
    "    # Formato el precio como float con dos decimales para mayor claridad\n",
    "    price_formatted = f\"{row['price']:.2f} pesos argentinos\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"Memorize the relationship between the 'title' and 'price' data of items in a Marketplace\\n\\n\"\n",
    "        f\"title: {row['title']}\\n\"\n",
    "        f\"price: {price_formatted}\\n\"  # Incluyo el precio formateado con dos decimales y la unidad\n",
    "        f\"CATEGORIA_CODE: {row['CATEGORIA_CODE']}\\n\"\n",
    "        f\"<sep>\\n\"\n",
    "    )\n",
    "\n",
    "    # Completo la categoría basada en el código de categoría\n",
    "    completion = \"Electrónica y audio\" if row['CATEGORIA_CODE'] == \"MLA1000\" else \"Otra Categoría\"\n",
    "\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "    # prompt = (\n",
    "    #     f\"Memoriza la relación entre la data de 'title' y 'price' de los artículos de un Marketplace \\n\\n\"\n",
    "    #     f\"title: {row['title']}\\n\"\n",
    "    #     f\"price: {row['price']}\\n\"\n",
    "    #     f\"CATEGORIA_CODE: {row['CATEGORIA_CODE']}\\n\"\n",
    "    #     f\"<sep>\\n\"\n",
    "    # )\n",
    "    # completion = \"Electrónica y audio\" if row['CATEGORIA_CODE'] == \"MLA1000\" else \"Otra Categoría\"\n",
    "    # return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "# Aplicar la función a cada fila\n",
    "formatted_data = df.apply(create_prompt_completion, axis=1)\n",
    "\n",
    "# Convertir a Dataset de Hugging Face\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(formatted_data.tolist()))\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Reducir el tamaño del conjunto de datos\n",
    "NUM_EJ_TRAIN = 1500\n",
    "NUM_EJ_VAL = 500\n",
    "NUM_EJ_TEST = 200\n",
    "\n",
    "# Subconjunto de entrenamiento\n",
    "split_datasets['train'] = split_datasets['train'].select(range(min(NUM_EJ_TRAIN, len(split_datasets['train']))))\n",
    "\n",
    "# Subconjunto de validación (utilizando parte del conjunto de prueba)\n",
    "split_datasets['validation'] = split_datasets['test'].select(range(min(NUM_EJ_VAL, len(split_datasets['test']))))\n",
    "\n",
    "# Subconjunto de pruebas\n",
    "split_datasets['test'] = split_datasets['test'].select(range(min(NUM_EJ_TEST, len(split_datasets['test']))))\n",
    "\n",
    "print(split_datasets)\n",
    "\n",
    "# Mostrar una muestra de los datos buenos y malos\n",
    "print(split_datasets['train'].to_pandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenización del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Tokenización y padding\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las cosas que comentabamos cuándo comenzamos a hablar de modelos generativos como la Redes Neuronales Recurrentes, es que este tipo de algoritmos, al igual que los LLMs, reciben secuencias del mismo tamaño.\n",
    "\n",
    "Con lo cual, al igual que hicimos en ese caso práctico al comienzo del curso, debemos obtener la secuencia más larga de nuestro conjunto de datos y realizar padding al resto de secuencias para que todas tengan el mismo tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 1318.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximo tamaño de prompt: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6000.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximo tamaño de completion: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Calculamos el tamaño máximo de prompt\n",
    "prompts_tokens = concatenate_datasets([split_datasets[\"train\"], split_datasets[\"validation\"], split_datasets[\"test\"]]).map(lambda x: tokenizer(x[\"prompt\"], truncation=True), batched=True)\n",
    "max_prompt_len = max([len(x) for x in prompts_tokens[\"input_ids\"]])\n",
    "print(f\"Maximo tamaño de prompt: {max_prompt_len}\")\n",
    "\n",
    "# Calculamos el tamaño máximo de completion\n",
    "completions_tokens = concatenate_datasets([split_datasets[\"train\"], split_datasets[\"validation\"], split_datasets[\"test\"]]).map(lambda x: tokenizer(x[\"completion\"], truncation=True), batched=True)\n",
    "max_completion_len = max([len(x) for x in completions_tokens[\"input_ids\"]])\n",
    "print(f\"Maximo tamaño de completion: {max_completion_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 2045.80 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1251.55 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1251.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "                                           input_ids  \\\n",
      "0  [23018, 52, 1737, 8, 1675, 344, 8, 3, 31, 2186...   \n",
      "1  [23018, 52, 1737, 8, 1675, 344, 8, 3, 31, 2186...   \n",
      "2  [23018, 52, 1737, 8, 1675, 344, 8, 3, 31, 2186...   \n",
      "3  [23018, 52, 1737, 8, 1675, 344, 8, 3, 31, 2186...   \n",
      "4  [23018, 52, 1737, 8, 1675, 344, 8, 3, 31, 2186...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                        labels  \n",
      "0  [3, 21543, 52, 15742, 2617, 3, 63, 2931, 1]  \n",
      "1  [3, 21543, 52, 15742, 2617, 3, 63, 2931, 1]  \n",
      "2  [3, 21543, 52, 15742, 2617, 3, 63, 2931, 1]  \n",
      "3  [3, 21543, 52, 15742, 2617, 3, 63, 2931, 1]  \n",
      "4  [3, 21543, 52, 15742, 2617, 3, 63, 2931, 1]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Función de tokenización y padding\n",
    "def padding_tokenizer(datos):\n",
    "    # Tokenizar inputs (prompts)\n",
    "    model_inputs = tokenizer(datos['prompt'], max_length=max_prompt_len, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Tokenizar labels (completions)\n",
    "    model_labels = tokenizer(datos['completion'], max_length=max_completion_len, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Sustituimos el caracter de padding de las completion por -100 para que no se tenga en cuenta en el entrenamiento\n",
    "    model_labels[\"input_ids\"] = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in model_labels[\"input_ids\"]]\n",
    "\n",
    "    model_inputs['labels'] = model_labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Aplicar tokenización y padding a los conjuntos de datos\n",
    "split_datasets = split_datasets.map(padding_tokenizer, batched=True, remove_columns=['prompt', 'completion'])\n",
    "\n",
    "# Verificar la estructura y algunas muestras de los datos tokenizados\n",
    "print(split_datasets)\n",
    "\n",
    "# Mostrar una muestra de los datos tokenizados\n",
    "print(split_datasets['train'].to_pandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Lectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Cargamos el modelo\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Evaluación durante el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación implementamos un conjunto de funciones auxiliares para evluar los resultados durante el proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SantiNR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metrica de evaluación\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Funciona auxiliar para preprocesar el texto\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum espera una nueva línea después de cada frase\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Reemplazamos -100 en las etiquetas porque no podemos decodificarlo\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Preprocesamos el texto\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Lectura y adaptación de los datos para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Ignoramos los tokens relacionados con el padding durante el proceso de entrenamiento para los prompts\n",
    "label_pad_token_id = -100\n",
    "\n",
    "# Recolector de datos para el entrenamiento del modelo\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Preparación y ejecución del fine-tuning (entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "REPOSITORY=\"flan-t5-small-fine-tuned-test50\"\n",
    "\n",
    "# Definimos las opciones del entrenamiento\n",
    "# Definimos las opciones del entrenamiento\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # Hiperprámetros del entrenamiento\n",
    "    output_dir=REPOSITORY,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=4,\n",
    "    # Estrategias de logging y evaluación\n",
    "    logging_dir=f\"{REPOSITORY}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Creamos la instancia de entrenamiento\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=split_datasets[\"train\"],  # Usar split_datasets en lugar de ds_tokens\n",
    "    eval_dataset=split_datasets[\"validation\"],  # Usar split_datasets en lugar de ds_tokens\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:13<00:34,  2.31s/it]c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                             \n",
      "\n",
      "                                              \n",
      " 25%|██▌       | 5/20 [00:16<00:34,  2.31s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5527853965759277, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 18.6, 'eval_runtime': 3.4736, 'eval_samples_per_second': 2.879, 'eval_steps_per_second': 0.576, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:26<00:21,  2.17s/it]c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                             \n",
      "\n",
      "                                               \n",
      " 50%|█████     | 10/20 [00:29<00:21,  2.17s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8520581722259521, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_gen_len': 16.8, 'eval_runtime': 2.8338, 'eval_samples_per_second': 3.529, 'eval_steps_per_second': 0.706, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:39<00:10,  2.09s/it]c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                             \n",
      "\n",
      "                                               \n",
      " 75%|███████▌  | 15/20 [00:42<00:10,  2.09s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4614403247833252, 'eval_rouge1': 2.2222, 'eval_rouge2': 0.0, 'eval_rougeL': 2.2222, 'eval_rougeLsum': 2.2222, 'eval_gen_len': 16.0, 'eval_runtime': 2.8896, 'eval_samples_per_second': 3.461, 'eval_steps_per_second': 0.692, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:52<00:00,  2.09s/it]c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                             \n",
      "\n",
      "                                               \n",
      "100%|██████████| 20/20 [00:54<00:00,  2.09s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3082988262176514, 'eval_rouge1': 1.8182, 'eval_rouge2': 0.0, 'eval_rougeL': 1.8182, 'eval_rougeLsum': 1.8182, 'eval_gen_len': 17.4, 'eval_runtime': 2.7258, 'eval_samples_per_second': 3.669, 'eval_steps_per_second': 0.734, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "\n",
      "100%|██████████| 20/20 [00:56<00:00,  2.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 56.1678, 'train_samples_per_second': 2.849, 'train_steps_per_second': 0.356, 'train_loss': 2.7374202728271486, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.7374202728271486, metrics={'train_runtime': 56.1678, 'train_samples_per_second': 2.849, 'train_steps_per_second': 0.356, 'total_flos': 6041441402880.0, 'train_loss': 2.7374202728271486, 'epoch': 4.0})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iniciar el entrenamiento del modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/flan-t5-small-fine-tuned-test50\\\\tokenizer_config.json',\n",
       " '../model/flan-t5-small-fine-tuned-test50\\\\special_tokens_map.json',\n",
       " '../model/flan-t5-small-fine-tuned-test50\\\\spiece.model',\n",
       " '../model/flan-t5-small-fine-tuned-test50\\\\added_tokens.json')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('../model/'+REPOSITORY)\n",
    "tokenizer.save_pretrained('../model/'+REPOSITORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la evaluación: {'eval_loss': 1.3082988262176514, 'eval_rouge1': 1.8182, 'eval_rouge2': 0.0, 'eval_rougeL': 1.8182, 'eval_rougeLsum': 1.8182, 'eval_gen_len': 17.4, 'eval_runtime': 3.6959, 'eval_samples_per_second': 2.706, 'eval_steps_per_second': 0.541, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de validación\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Resultados de la evaluación: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SantiNR\\mplace_meli\\venv_meli\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: $9.99\n"
     ]
    }
   ],
   "source": [
    "# Define a custom prompt for testing\n",
    "test_prompt = \"Cuál es el price del iphone\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate a response\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    max_length=max_completion_len,  # You can set this to an appropriate length\n",
    "    num_return_sequences=1,  # Number of generated sequences\n",
    "    no_repeat_ngram_size=2,  # Prevent repetition\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated sequence\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text (12 characters): £20\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Verificar si MPS (Metal Performance Shaders) está disponible y usarlo si es posible\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Tokenización y padding\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')\n",
    "\n",
    "# Cargamos el modelo\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "\n",
    "# Función para generar texto con una longitud específica\n",
    "def generate_text(prompt, max_length=18):\n",
    "    # Tokenizar el prompt de entrada\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generar una secuencia de salida con la longitud máxima deseada\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        max_length=max_length,  # Aquí especificamos la longitud máxima deseada\n",
    "        num_return_sequences=1,  # Número de secuencias generadas\n",
    "        no_repeat_ngram_size=2,  # Evitar repeticiones\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decodificar la secuencia generada\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Ejemplo de uso\n",
    "#test_prompt = \"Cuál es el price del iphone?\"\n",
    "#test_prompt = \"Cuál es el price del title parlante?\"\n",
    "test_prompt = \"What is the price of the product with title 'Auriculares' in Argentine pesos?\"\n",
    "\n",
    "\n",
    "generated_text = generate_text(test_prompt, max_length=12)\n",
    "print(\"Generated Text (12 characters):\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: £30\n",
      "Extracted Price: 30\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Verificar si MPS (Metal Performance Shaders) está disponible y usarlo si es posible\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Tokenización y padding\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')\n",
    "\n",
    "# Cargamos el modelo\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "\n",
    "# Función para generar texto con una longitud específica\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Tokenizar el prompt de entrada\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generar una secuencia de salida con la longitud máxima deseada\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        max_length=max_length,  # Aquí especificamos la longitud máxima deseada\n",
    "        num_return_sequences=1,  # Número de secuencias generadas\n",
    "        no_repeat_ngram_size=2,  # Evitar repeticiones\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decodificar la secuencia generada\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Ejemplo de uso con un prompt más específico\n",
    "test_prompt = \"Price of 'Auriculares In-ear Inalámbricos'?\"\n",
    "#test_prompt = \"What is the 'CATEGORIA_CODE' of the product 'Parlante'?\"\n",
    "\n",
    "\n",
    "generated_text = generate_text(test_prompt, max_length=50)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "\n",
    "# Implementar lógica para extraer la información relevante del texto generado\n",
    "def parse_generated_text(generated_text):\n",
    "    # Regular expression pattern to find numbers (including decimals)\n",
    "    number_pattern = r'(\\d+(\\.\\d+)?)'\n",
    "\n",
    "    # Search for the number in the generated text\n",
    "    match = re.search(number_pattern, generated_text)\n",
    "    if match:\n",
    "        extracted_price = match.group(0)  # Get the matched number as a string\n",
    "        return extracted_price\n",
    "    else:\n",
    "        return \"Price not found\"  # Handle case where price is not found\n",
    "\n",
    "# Ejemplo de cómo podrías usar la función parse_generated_text\n",
    "price_answer = parse_generated_text(generated_text)\n",
    "print(\"Extracted Price:\", price_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>CATEGORIA_CODE</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Auriculares Bluetooth In-ear Gamer F9-5 Negro ...</td>\n",
       "      <td>7599.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auriculares Inalámbricos Jbl Tune 520bt Negro</td>\n",
       "      <td>68999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Auriculares In-ear Inalámbricos Xiaomi Redmi B...</td>\n",
       "      <td>33894.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parlante Philco Djp11p Portátil Con Bluetooth ...</td>\n",
       "      <td>59999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pila Aa Energizer Max E91 Cilíndrica - Pack De...</td>\n",
       "      <td>4200.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Google Chromecast 4 Generación Con Google Tv H...</td>\n",
       "      <td>122795.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chromecast Google Ga03131 Tv Hd 8gb 2gb Ram 60...</td>\n",
       "      <td>101000.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Auriculares In-ear Gamer Inalámbricos Tws F9-5...</td>\n",
       "      <td>7599.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20 X Pilas Aaa Energizer Max E92 Pilas Alcalin...</td>\n",
       "      <td>12700.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Televisor Tcl Led 32s5400af Android Tv 32 Full...</td>\n",
       "      <td>295999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Auriculares In-ear Inalámbricos Vohz Bluetooth...</td>\n",
       "      <td>5999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Auriculares In-ear Inalámbricos Vohz Bluetooth...</td>\n",
       "      <td>5999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4 X Pilas Aaa Energizer Max E92 Pilas Alcalina...</td>\n",
       "      <td>2868.43</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Auriculares M10 Pro | Hi Fi | Power Bank Color...</td>\n",
       "      <td>5900.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Auriculares In-ear Inalámbricos A6s Negro</td>\n",
       "      <td>5719.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20 X Pilas Aa Energizer Max E91 Pilas Alcalina...</td>\n",
       "      <td>15409.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chromecast Google Ga03131 Tv Hd 8gb 2gb Ram 60...</td>\n",
       "      <td>79599.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Auricular In-ear Gamer Inalámbrico Fan Pro F10...</td>\n",
       "      <td>7377.60</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Soporte Electroland Sop 14-55 De Pared Para Tv...</td>\n",
       "      <td>14999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Smart Tv Samsung 55 Cristal Uhd 4k Cu7000</td>\n",
       "      <td>949999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Auriculares In Ear Jbl Tune 110 Black</td>\n",
       "      <td>13490.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Auriculares In-ear Gamer Inalámbricos Alpina G...</td>\n",
       "      <td>7404.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Auriculares In-ear Inalámbricos Vohz Bluetooth...</td>\n",
       "      <td>7125.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cable Hdmi 3 Metros V1.4 Full Hd 4k Dorado Led...</td>\n",
       "      <td>3475.21</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Auriculares Inalámbrico Qcy T1c Color Negro In...</td>\n",
       "      <td>25049.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cable Conversor Hdmi A Vga Conversor Netbook 1...</td>\n",
       "      <td>4503.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pila Aa Scp Recargables Ni-mh-2700 Cilíndrica ...</td>\n",
       "      <td>13699.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Pila Aaa Eveready Super Heavy Duty 1212 Cilínd...</td>\n",
       "      <td>1140.30</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Auriculares Inalámbricos Bluetooth Lenovo Lp40...</td>\n",
       "      <td>21132.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Batería Pila Cr2032 Pack De 5 Pilas 3v. 220 Mah</td>\n",
       "      <td>3465.60</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Pilas Cr2032 Energizer Lithium Coin - Blister ...</td>\n",
       "      <td>4965.84</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1 X Pila Batería 9v Energizer Max 522 Pila Alc...</td>\n",
       "      <td>6665.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Google Chromecast 3 Full Hd Media Streaming Negro</td>\n",
       "      <td>53700.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Pila Noga Recargable Np-aam 2.7b Cilíndrica - ...</td>\n",
       "      <td>6200.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4 X Pilas Aa Energizer Max E91 Pilas Alcalinas...</td>\n",
       "      <td>2868.43</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Microfono Corbatero Inalambrico Compatible iPh...</td>\n",
       "      <td>22862.40</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Auriculares Bluetooth M10 Superior F9-5 Carga ...</td>\n",
       "      <td>6299.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Auriculares In-ear Inalámbricos Vohz Bluetooth...</td>\n",
       "      <td>5949.15</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Auriculares In-ear Gamer Inalámbricos Tws F9-5...</td>\n",
       "      <td>5999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Auriculares In-ear Gamer Inalámbricos Soundpea...</td>\n",
       "      <td>142799.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2 X Pilas Recargables Aa Energizer Recharge Nh...</td>\n",
       "      <td>8499.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Soporte Tv/monitor Force L40-222 - Negro</td>\n",
       "      <td>17399.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Soporte Shirka Sk-471 De Pared Para Tv/monitor...</td>\n",
       "      <td>8999.40</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Parlante Spica Sp-4408 Bluetooth Portatil 8 Pu...</td>\n",
       "      <td>99999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Bateria 9v Super Heavy Duty Fulltotal Controle...</td>\n",
       "      <td>2123.50</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Cable Hdmi 1.8 Metros Led Pc Monitor</td>\n",
       "      <td>2368.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Control Remoto 4k Para Smart Tv Led Los Modelo...</td>\n",
       "      <td>3869.68</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Soporte Led Tv Lcd Smart Articulado Doble Braz...</td>\n",
       "      <td>99999.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Soporte Tv Televisor 50 Pulgadas Lcd Led Pared...</td>\n",
       "      <td>7699.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Soporte Tv Televisor 50 Pulgadas Lcd Led Pared...</td>\n",
       "      <td>7699.00</td>\n",
       "      <td>MLA1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title      price  \\\n",
       "0   Auriculares Bluetooth In-ear Gamer F9-5 Negro ...    7599.00   \n",
       "1       Auriculares Inalámbricos Jbl Tune 520bt Negro   68999.00   \n",
       "2   Auriculares In-ear Inalámbricos Xiaomi Redmi B...   33894.00   \n",
       "3   Parlante Philco Djp11p Portátil Con Bluetooth ...   59999.00   \n",
       "4   Pila Aa Energizer Max E91 Cilíndrica - Pack De...    4200.00   \n",
       "5   Google Chromecast 4 Generación Con Google Tv H...  122795.00   \n",
       "6   Chromecast Google Ga03131 Tv Hd 8gb 2gb Ram 60...  101000.00   \n",
       "7   Auriculares In-ear Gamer Inalámbricos Tws F9-5...    7599.00   \n",
       "8   20 X Pilas Aaa Energizer Max E92 Pilas Alcalin...   12700.00   \n",
       "9   Televisor Tcl Led 32s5400af Android Tv 32 Full...  295999.00   \n",
       "10  Auriculares In-ear Inalámbricos Vohz Bluetooth...    5999.00   \n",
       "11  Auriculares In-ear Inalámbricos Vohz Bluetooth...    5999.00   \n",
       "12  4 X Pilas Aaa Energizer Max E92 Pilas Alcalina...    2868.43   \n",
       "13  Auriculares M10 Pro | Hi Fi | Power Bank Color...    5900.00   \n",
       "14          Auriculares In-ear Inalámbricos A6s Negro    5719.00   \n",
       "15  20 X Pilas Aa Energizer Max E91 Pilas Alcalina...   15409.00   \n",
       "16  Chromecast Google Ga03131 Tv Hd 8gb 2gb Ram 60...   79599.00   \n",
       "17  Auricular In-ear Gamer Inalámbrico Fan Pro F10...    7377.60   \n",
       "18  Soporte Electroland Sop 14-55 De Pared Para Tv...   14999.00   \n",
       "19          Smart Tv Samsung 55 Cristal Uhd 4k Cu7000  949999.00   \n",
       "20              Auriculares In Ear Jbl Tune 110 Black   13490.00   \n",
       "21  Auriculares In-ear Gamer Inalámbricos Alpina G...    7404.00   \n",
       "22  Auriculares In-ear Inalámbricos Vohz Bluetooth...    7125.00   \n",
       "23  Cable Hdmi 3 Metros V1.4 Full Hd 4k Dorado Led...    3475.21   \n",
       "24  Auriculares Inalámbrico Qcy T1c Color Negro In...   25049.00   \n",
       "25  Cable Conversor Hdmi A Vga Conversor Netbook 1...    4503.00   \n",
       "26  Pila Aa Scp Recargables Ni-mh-2700 Cilíndrica ...   13699.00   \n",
       "27  Pila Aaa Eveready Super Heavy Duty 1212 Cilínd...    1140.30   \n",
       "28  Auriculares Inalámbricos Bluetooth Lenovo Lp40...   21132.00   \n",
       "29    Batería Pila Cr2032 Pack De 5 Pilas 3v. 220 Mah    3465.60   \n",
       "30  Pilas Cr2032 Energizer Lithium Coin - Blister ...    4965.84   \n",
       "31  1 X Pila Batería 9v Energizer Max 522 Pila Alc...    6665.00   \n",
       "32  Google Chromecast 3 Full Hd Media Streaming Negro   53700.00   \n",
       "33  Pila Noga Recargable Np-aam 2.7b Cilíndrica - ...    6200.00   \n",
       "34  4 X Pilas Aa Energizer Max E91 Pilas Alcalinas...    2868.43   \n",
       "35  Microfono Corbatero Inalambrico Compatible iPh...   22862.40   \n",
       "36  Auriculares Bluetooth M10 Superior F9-5 Carga ...    6299.00   \n",
       "37  Auriculares In-ear Inalámbricos Vohz Bluetooth...    5949.15   \n",
       "38  Auriculares In-ear Gamer Inalámbricos Tws F9-5...    5999.00   \n",
       "39  Auriculares In-ear Gamer Inalámbricos Soundpea...  142799.00   \n",
       "40  2 X Pilas Recargables Aa Energizer Recharge Nh...    8499.00   \n",
       "41           Soporte Tv/monitor Force L40-222 - Negro   17399.00   \n",
       "42  Soporte Shirka Sk-471 De Pared Para Tv/monitor...    8999.40   \n",
       "43  Parlante Spica Sp-4408 Bluetooth Portatil 8 Pu...   99999.00   \n",
       "44  Bateria 9v Super Heavy Duty Fulltotal Controle...    2123.50   \n",
       "45               Cable Hdmi 1.8 Metros Led Pc Monitor    2368.00   \n",
       "46  Control Remoto 4k Para Smart Tv Led Los Modelo...    3869.68   \n",
       "47  Soporte Led Tv Lcd Smart Articulado Doble Braz...   99999.00   \n",
       "48  Soporte Tv Televisor 50 Pulgadas Lcd Led Pared...    7699.00   \n",
       "49  Soporte Tv Televisor 50 Pulgadas Lcd Led Pared...    7699.00   \n",
       "\n",
       "   CATEGORIA_CODE  label  \n",
       "0         MLA1000      1  \n",
       "1         MLA1000      1  \n",
       "2         MLA1000      1  \n",
       "3         MLA1000      1  \n",
       "4         MLA1000      1  \n",
       "5         MLA1000      1  \n",
       "6         MLA1000      1  \n",
       "7         MLA1000      1  \n",
       "8         MLA1000      1  \n",
       "9         MLA1000      1  \n",
       "10        MLA1000      1  \n",
       "11        MLA1000      1  \n",
       "12        MLA1000      1  \n",
       "13        MLA1000      1  \n",
       "14        MLA1000      1  \n",
       "15        MLA1000      1  \n",
       "16        MLA1000      1  \n",
       "17        MLA1000      1  \n",
       "18        MLA1000      1  \n",
       "19        MLA1000      1  \n",
       "20        MLA1000      1  \n",
       "21        MLA1000      1  \n",
       "22        MLA1000      1  \n",
       "23        MLA1000      1  \n",
       "24        MLA1000      1  \n",
       "25        MLA1000      1  \n",
       "26        MLA1000      1  \n",
       "27        MLA1000      1  \n",
       "28        MLA1000      1  \n",
       "29        MLA1000      1  \n",
       "30        MLA1000      1  \n",
       "31        MLA1000      1  \n",
       "32        MLA1000      1  \n",
       "33        MLA1000      1  \n",
       "34        MLA1000      1  \n",
       "35        MLA1000      1  \n",
       "36        MLA1000      1  \n",
       "37        MLA1000      1  \n",
       "38        MLA1000      1  \n",
       "39        MLA1000      1  \n",
       "40        MLA1000      1  \n",
       "41        MLA1000      1  \n",
       "42        MLA1000      1  \n",
       "43        MLA1000      1  \n",
       "44        MLA1000      1  \n",
       "45        MLA1000      1  \n",
       "46        MLA1000      1  \n",
       "47        MLA1000      1  \n",
       "48        MLA1000      1  \n",
       "49        MLA1000      1  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
